{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import models_vit\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import timm\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_cam(model, model_type, target_layers, data_lines):\n",
    "    for line in data_lines:\n",
    "        img_path = line.split(' ')[0]\n",
    "        model.eval()\n",
    "        rgb_img = cv2.imread(data_dir + img_path, 1)[:, :, ::-1]\n",
    "\n",
    "        rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "        rgb_img = np.float32(rgb_img) / 255\n",
    "        input_tensor = preprocess_image(rgb_img, mean = [0.5056, 0.5056, 0.5056], std = [0.252, 0.252, 0.252])\n",
    "\n",
    "        cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform, use_cuda=1)\n",
    "        targets = None\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=False)\n",
    "        cv2.imwrite(model_type + img_path, visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_image(model, target_layers, data_lines, output_file_path):\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with open(output_file_path, 'a') as output_file:\n",
    "        for line in lines:\n",
    "            line_split = line.split()\n",
    "            img_path = line_split[0]\n",
    "            imageLabel = line_split[1:num_class + 1]\n",
    "            imageLabel = [float(i) for i in imageLabel]\n",
    "            imageLabel = [imageLabel]\n",
    "            imageLabel = torch.tensor(imageLabel).to(torch.device('cuda'))\n",
    "\n",
    "            model.eval()\n",
    "            rgb_img = cv2.imread(data_dir + img_path, 1)[:, :, ::-1]\n",
    "\n",
    "            rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "            rgb_img = np.float32(rgb_img) / 255\n",
    "            input_tensor = preprocess_image(rgb_img, mean = [0.5056, 0.5056, 0.5056], std = [0.252, 0.252, 0.252])\n",
    "\n",
    "            output, _ = model(input_tensor.cuda())\n",
    "            acc, res = accuracy(imageLabel, output.sigmoid(), num_class)\n",
    "        \n",
    "            output_file.write(f'image_path: {img_path}, acc_each_class: {res}, acc_avg: {acc}')\n",
    "        \n",
    "            \n",
    "#     outputs = torch.cat(outputs, dim = 0).sigmoid().cpu().numpy()\n",
    "#     targets = torch.cat(targets, dim = 0).cpu().numpy()\n",
    "    \n",
    "#     auc = computeAUROC(targets, outputs, num_class)\n",
    "#     auc_each_class_array = np.array(auc)\n",
    "#     missing_classes_index = np.where(auc_each_class_array == 0)[0]\n",
    "#     # print(missing_classes_index)\n",
    "#     if missing_classes_index.shape[0] > 0:\n",
    "#         print('There are classes that not be predicted during testing,'\n",
    "#               ' the indexes are:', missing_classes_index)\n",
    "\n",
    "#     auc_avg = np.average(auc_each_class_array[auc_each_class_array != 0])\n",
    "#     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAUROC(dataGT, dataPRED, classCount):\n",
    "    outAUROC = []\n",
    "    # print(dataGT.shape, dataPRED.shape)\n",
    "    for i in range(classCount):\n",
    "        try:\n",
    "            outAUROC.append(roc_auc_score(dataGT[:, i], dataPRED[:, i]))\n",
    "        except:\n",
    "            outAUROC.append(0.)\n",
    "    print(outAUROC)\n",
    "    return outAUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dataGT, dataPRED, classCount):\n",
    "    acc = 0\n",
    "    pred = (dataPRED >= 0.5).float()\n",
    "    correct = pred.eq(dataGT[0])\n",
    "    res = correct.int()\n",
    "    return torch.sum(correct).item() * 100 / classCount, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/data/yyang409/rgoel15/ChestX-ray14/images/\"\n",
    "img_dir = \"data_splits/chestxray/grad_cam_test.txt\"\n",
    "\n",
    "out_dir = '/data/yyang409/rgoel15/medical_mae_soft_low_rank/grad-cam-images/'\n",
    "\n",
    "base_model = out_dir + 'base_model/'\n",
    "low_rank = out_dir + 'low_rank_model/'\n",
    "\n",
    "base_auc_out = base_model + 'base.txt'\n",
    "low_auc_out = low_rank + 'low.txt'\n",
    "\n",
    "num_class = 14\n",
    "\n",
    "with open(img_dir, 'r') as input_file:\n",
    "    lines = input_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAD CAM and EVAL BASE MODEL\n",
    "model = models_vit.__dict__['vit_base_patch16'](img_size=224,\n",
    "                                            num_classes=14,\n",
    "                                            drop_rate=0,\n",
    "                                            drop_path_rate=0,\n",
    "                                            global_pool=True)\n",
    "checkpoint = torch.load('/data/yyang409/rgoel15/medical_mae_original_models/finetuned_models/vit-b_CXR_0.5M_mae.pth')\n",
    "model.load_state_dict(checkpoint['model'], strict=True)\n",
    "model.to(torch.device('cuda'))\n",
    "target_layers = [model.blocks[-1].norm1]\n",
    "\n",
    "# run_grad_cam(model, base_model, target_layers, lines)\n",
    "eval_image(model, target_layers, lines, base_auc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7868745938921378, 0.9556737588652482, 0.8418749999999999, 0.6768, 0.6955555555555555, 0.5326460481099656, 0.0, 0.9044444444444445, 0.8179347826086957, 0.7835051546391751, 0.9452631578947368, 0.9243986254295532, 0.7801418439716312, 1.0]\n",
      "There are classes that not be predicted during testing, the indexes are: [6]\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "# GRAD CAM and EVAL LOW RANK MODEL\n",
    "model = models_vit.__dict__['vit_base_patch16'](img_size=224,\n",
    "                                            num_classes=14,\n",
    "                                            drop_rate=0,\n",
    "                                            drop_path_rate=0,\n",
    "                                            global_pool=True)\n",
    "checkpoint = torch.load('/data/yyang409/rgoel15/medical_mae_low_rank/vit_b/checkpoint-best_auc_38_0.00125_20.pth')\n",
    "model.load_state_dict(checkpoint['model'], strict=True)\n",
    "target_layers = [model.blocks[-1].norm1]\n",
    "\n",
    "run_grad_cam(model, low_rank, target_layers, lines)\n",
    "eval_image(model, target_layers, lines, low_auc_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "medical"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
