{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import models_vit\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import timm\n",
    "import matplotlib.pylab as plt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_cam(model, model_type, target_layers, data_lines):\n",
    "    model.eval()\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform, use_cuda=1)\n",
    "    for line in data_lines:\n",
    "        img_path = line.split(' ')[0]\n",
    "        rgb_img = cv2.imread(data_dir + img_path, 1)[:, :, ::-1]\n",
    "        rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "        rgb_img = np.float32(rgb_img) / 255\n",
    "        input_tensor = preprocess_image(rgb_img, mean = [0.5056, 0.5056, 0.5056], std = [0.252, 0.252, 0.252])\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=None)\n",
    "\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=False)\n",
    "        cv2.imwrite(model_type + img_path, visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_image(model, target_layers, data_lines, output_file_path):\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with open(output_file_path, 'a') as output_file:\n",
    "        for line in lines:\n",
    "            line_split = line.split()\n",
    "            img_path = line_split[0]\n",
    "            imageLabel = line_split[1:num_class + 1]\n",
    "            imageLabel = [float(i) for i in imageLabel]\n",
    "            imageLabel = [imageLabel]\n",
    "            imageLabel = torch.tensor(imageLabel).to(torch.device('cuda'))\n",
    "\n",
    "            model.eval()\n",
    "            rgb_img = cv2.imread(data_dir + img_path, 1)[:, :, ::-1]\n",
    "\n",
    "            rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "            rgb_img = np.float32(rgb_img) / 255\n",
    "            input_tensor = preprocess_image(rgb_img, mean = [0.5056, 0.5056, 0.5056], std = [0.252, 0.252, 0.252])\n",
    "\n",
    "            output, _ = model(input_tensor.cuda())\n",
    "            outputs.append(output)\n",
    "            targets.append(imageLabel)\n",
    "            acc, res = accuracy(imageLabel, output.sigmoid(), num_class)\n",
    "        \n",
    "            output_file.write(f'{img_path}, acc_each_class: {res}, acc_avg: {acc}\\n')\n",
    "        \n",
    "            \n",
    "    outputs = torch.cat(outputs, dim = 0).sigmoid().cpu().numpy()\n",
    "    targets = torch.cat(targets, dim = 0).cpu().numpy()\n",
    "    \n",
    "    auc = computeAUROC(targets, outputs, num_class)\n",
    "    auc_each_class_array = np.array(auc)\n",
    "    missing_classes_index = np.where(auc_each_class_array == 0)[0]\n",
    "    # print(missing_classes_index)\n",
    "    if missing_classes_index.shape[0] > 0:\n",
    "        print('There are classes that not be predicted during testing,'\n",
    "              ' the indexes are:', missing_classes_index)\n",
    "\n",
    "    auc_avg = np.average(auc_each_class_array[auc_each_class_array != 0])\n",
    "    print(auc_avg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAUROC(dataGT, dataPRED, classCount):\n",
    "    outAUROC = []\n",
    "    # print(dataGT.shape, dataPRED.shape)\n",
    "    for i in range(classCount):\n",
    "        try:\n",
    "            outAUROC.append(roc_auc_score(dataGT[:, i], dataPRED[:, i]))\n",
    "        except:\n",
    "            outAUROC.append(0.)\n",
    "    print(outAUROC)\n",
    "    return outAUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dataGT, dataPRED, classCount):\n",
    "    acc = 0\n",
    "    pred = (dataPRED >= 0.5).float()\n",
    "    correct = pred.eq(dataGT[0])\n",
    "    res = correct.int()\n",
    "    return torch.sum(correct).item() * 100 / classCount, res.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/path/to/dataset\"\n",
    "img_dir = \"./data_splits/chestxray/test_official.txt\"\n",
    "\n",
    "out_dir = '/path/to/output_dir'\n",
    "\n",
    "base_model = out_dir + 'base_model/'\n",
    "low_rank = out_dir + 'low_rank_model/'\n",
    "\n",
    "base_auc_out = base_model + 'base.txt'\n",
    "low_auc_out = low_rank + 'low.txt'\n",
    "\n",
    "num_class = 14\n",
    "\n",
    "with open(img_dir, 'r') as input_file:\n",
    "    lines = input_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7827995028467994, 0.9053250454635379, 0.8444961162607254, 0.7041887629842073, 0.8292594733272587, 0.7491918601484793, 0.7383613396552178, 0.8690073974268019, 0.7637604296227873, 0.8653487024375923, 0.9312777168387507, 0.841948746623469, 0.8129848719018493, 0.9090201744869773]\n",
      "0.824783581430318\n"
     ]
    }
   ],
   "source": [
    "# GRAD CAM and EVAL BASE MODEL\n",
    "model = models_vit.__dict__['vit_base_patch16'](img_size=224,\n",
    "                                            num_classes=14,\n",
    "                                            drop_rate=0,\n",
    "                                            drop_path_rate=0,\n",
    "                                            global_pool=True)\n",
    "checkpoint = torch.load('/path/to/base_model_checkpoint')\n",
    "model.load_state_dict(checkpoint['model'], strict=True)\n",
    "model.to(torch.device('cuda'))\n",
    "target_layers = [model.blocks[-1].norm1]\n",
    "\n",
    "run_grad_cam(model, base_model, target_layers, lines)\n",
    "eval_image(model, target_layers, lines, base_auc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7838100123285259, 0.9046906669700557, 0.8452978043376627, 0.7092338434355051, 0.8298423866025637, 0.7548728893223196, 0.739892450255455, 0.8692214746859062, 0.7671564203337086, 0.8653401794644549, 0.9310669018663773, 0.8479242871311056, 0.8118228233575656, 0.921626266033384]\n",
      "0.8272713147231849\n"
     ]
    }
   ],
   "source": [
    "# GRAD CAM and EVAL LOW RANK MODEL\n",
    "model = models_vit.__dict__['vit_base_patch16'](img_size=224,\n",
    "                                            num_classes=14,\n",
    "                                            drop_rate=0,\n",
    "                                            drop_path_rate=0,\n",
    "                                            global_pool=True)\n",
    "checkpoint = torch.load('/path/to/low_rank_model_checkpoint')\n",
    "model.load_state_dict(checkpoint['model'], strict=True)\n",
    "target_layers = [model.blocks[-1].norm1]\n",
    "\n",
    "run_grad_cam(model, low_rank, target_layers, lines)\n",
    "eval_image(model, target_layers, lines, low_auc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ...\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths\n",
    "file1_path = base_auc_out # Replace with the path to the first file\n",
    "file2_path = low_auc_out  # Replace with the path to the second file\n",
    "output_file_path = out_dir + 'result.txt'  # Replace with the desired output file path\n",
    "\n",
    "# Initialize dictionaries to store the 'C' values from both files\n",
    "file1_c_values = {}\n",
    "file2_c_values = {}\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Read and process data from the first file\n",
    "with open(file1_path, 'r') as file1:\n",
    "    for line in file1:\n",
    "        parts = line.strip().split(',')\n",
    "        if len(parts) == 3:\n",
    "            a, b, c = parts\n",
    "            file1_c_values[a] = float(c.split(':')[-1])\n",
    "\n",
    "# Read and process data from the second file\n",
    "with open(file2_path, 'r') as file2:\n",
    "    for line in file2:\n",
    "        parts = line.strip().split(',')\n",
    "        if len(parts) == 3:\n",
    "            a, b, c = parts\n",
    "            file2_c_value = float(c.split(':')[-1])\n",
    "            if a in file1_c_values and file2_c_value > file1_c_values[a]:\n",
    "                result_line = f\"image_path: {a}, base_acc: {file1_c_values[a]}, low_acc: {file2_c_value}\"\n",
    "                results.append(result_line)\n",
    "\n",
    "# Write the results to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for result_line in results:\n",
    "        output_file.write(result_line + '\\n')\n",
    "\n",
    "print(f\"Results saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Step 1: Read data from all four files into dictionaries\n",
    "file1_data = []\n",
    "file2_data = {}\n",
    "file3_data = {}\n",
    "file4_data = {}\n",
    "\n",
    "# Read data from the first file (name, a:A, b:B)\n",
    "with open(out_dir + 'result.txt', 'r') as file1:\n",
    "    for line in file1:\n",
    "        parts = line.strip().split(', ')\n",
    "        name = parts[0].split(': ')[1]\n",
    "        file1_data.append(name)\n",
    "\n",
    "# Read data from the second file (name, c:C)\n",
    "with open(base_auc_out, 'r') as file2:\n",
    "    for line in file2:\n",
    "        parts = line.strip().split(', ')\n",
    "        name = parts[0]\n",
    "        preds = parts[1].split(':')[1]\n",
    "        match = re.search(r'\\[\\[(.*?)\\]\\]', preds)\n",
    "        if match:\n",
    "            numeric_data = match.group(1).strip().split()\n",
    "            numeric_array = [int(val) for val in numeric_data]\n",
    "\n",
    "        file2_data[name] = numeric_array\n",
    "        \n",
    "\n",
    "# Read data from the third file (name, d:D)\n",
    "with open(low_auc_out, 'r') as file3:\n",
    "    for line in file3:\n",
    "        parts = line.strip().split(', ')\n",
    "        name = parts[0]\n",
    "        preds = parts[1].split(':')[1]\n",
    "        match = re.search(r'\\[\\[(.*?)\\]\\]', preds)\n",
    "        if match:\n",
    "            numeric_data = match.group(1).strip().split()\n",
    "            numeric_array = [int(val) for val in numeric_data]\n",
    "\n",
    "        file3_data[name] = numeric_array\n",
    "\n",
    "# Read data from the fourth file (name data)\n",
    "with open('./data_splits/chestxray/test_official.txt', 'r') as file4:\n",
    "    for line in file4:\n",
    "        lineItems = line.split()\n",
    "        imagePath = lineItems[0]\n",
    "        imageLabel = lineItems[1:15]\n",
    "        imageLabel = [int(i) for i in imageLabel]\n",
    "        file4_data[imagePath] = imageLabel\n",
    "\n",
    "def get_preds(acc_each_class, gt):\n",
    "    preds = []\n",
    "    for i in range(len(gt)):\n",
    "        if acc_each_class[i] == 1:\n",
    "            preds.append(gt[i])\n",
    "        else:\n",
    "            preds.append(1) if gt[i] == 0 else preds.append(0)\n",
    "    return preds\n",
    "    \n",
    "    \n",
    "# Step 2-4: Process the data and create the new format\n",
    "output_data = []\n",
    "for name in file1_data:\n",
    "    if name in file2_data and name in file3_data and name in file4_data:\n",
    "        c1 = file2_data[name]\n",
    "        c2 = file3_data[name]\n",
    "        data = file4_data[name]\n",
    "        preds_c1 = get_preds(c1, data)\n",
    "        preds_c2 = get_preds(c2, data)\n",
    "        new_line = f\"{name}, base: {preds_c1}, low_rank: {preds_c2}, GT: {data}\"\n",
    "        output_data.append(new_line)\n",
    "\n",
    "# Step 5: Write the new formatted data to a fifth file\n",
    "with open(out_dir + 'new_results.txt', 'w') as file5:\n",
    "    file5.writelines('\\n'.join(output_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "medical"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
